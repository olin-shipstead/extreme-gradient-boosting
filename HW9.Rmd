---
title: "HW9"
author: "Olin Shipstead"
date: "April 21, 2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

# Question 1

> *Analyze the agaricus dataset using extreme gradient boosting. This dataset is supplied with the xgboost package and is already divided into training and test subsets. The training and test data is in sparse matrix format (class dgCMatrix with 6 slots) while the labels (i.e. responses) are numeric vectors. You will have to decide whether to use this data in sparse or dense format when you call xgboost. Label is binary and represents whether a mushroom is safe to eat, or not, given characteristics of the mushrooms (i.e., the predictors). Tune your model in the training phase to yield best performance. Then test your trained model using the test data. Show the predictive performance using tabular and graphical representations.*

First, I will import the xgboost package and take a peek at the agaricus dataset it contains:


```{r}
library(dplyr, quietly = T)
library(xgboost)

data("agaricus.test")
data("agaricus.train")

train <- agaricus.train
test <- agaricus.test

str(train)
str(test)

```

Inspecting the data via the str function, I see that each data set contains data in matrix form and labels in binary form. The matrix within the train set will serve as the predictor variables for which we will train our boosting model to predict the binary label variable. There are 6513 observations in the training set and 1611 observations in the test set. Knowing this, we can tune an extreme gradient boosting model using the caret package:


```{r}
library(caret)

set.seed(3)
trainY <- ifelse(train$label == 0, "Negative","Positive")

XG <- train(x = train$data, 
            y = trainY,
            method="xgbTree",
            metric="ROC",
            trControl=trainControl(method = "repeatedcv",
                                   number = 3,
                                   summaryFunction=twoClassSummary,	                     
                                   classProbs=TRUE,
                                   allowParallel = TRUE),
            tuneGrid=expand.grid(eta=c(0.7), 
                                 max_depth = c(1, 10), 
                                 gamma = 10, 
                                 subsample = c(0.5, 1), 
                                 nrounds = 50, 
                                 colsample_bytree = c(0.5, 0.9), 
                                 min_child_weight = 1))
XG


```

According to the tuning from the caret package, the best parameter selection of the ones I provided is the one with the highest ROC value. The output above shows that all of the parameter combinations returned ROC values greater than 0.999, with the best model returning a perfect 1.000. This means that the extreme gradient boosting model with those parameters performed perfect classification on the training set. 

We can then determine the performance of that model on the testing data:

```{r}
pred <- predict(XG, test$data, type = "prob")[,2]
pred <- ifelse(pred < 0.5, 0,1)

table(pred, test$label)

```

Even using the test data, the tuned extreme gradient boosting model performs perfect classification. This remarkable outcome can be visualized using the ROC curve below:

```{r}

library(ROCR)
library(ggplot2)

pr <- predict(XG, test$data, type = "prob")[,2]
pred <- prediction(pr,test$label)
pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve - Tuned XGBoost [test]")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,label=paste("AUC =", round(au, 2)))
print(p)

```

As expected with a perfect model, the ROC curve follows the outside edges of the plot and the area under the curve is 1. The fact that the model does not misclassify a single point is somewhat suspicious from a modeler's perspective, and may warrant further study. 



# Question 2

> *A combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the vacuum is collected from and has effect on the Steam Turbine, the other three of the ambient variables affect the GT performance. The dataset (Folds5x2_pp.xlsx in Data Archive > Boosting) contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant. Use random forests and extreme gradient boosting to analyze this data. Create training and test data sets, tune your trained models using caret and evaluate test performance in both tabular and graphical formats.*

I will begin by downloading the data and taking a peek:

```{r}
ccpp <- read.csv("Folds5x2_pp.csv")

head(ccpp)
```

As expected, there are five columns of type double, four of which are predictors (AT, V, AP, RH) and one of which is response (EP). I will now begin building a random forest model as follows:

```{r}
library(randomForest, quietly=T)

set.seed(35)
sel <- sample(nrow(ccpp),nrow(ccpp)*0.6) # create selection for training set

rf <- randomForest(EP~., data=ccpp, subset=sel, importance=T, mtry=1)
rf_pred <- predict(rf, ccpp[-sel,])

rf_mse <- mean((rf_pred-ccpp[-sel,"EP"])^2)
rf_mse

```

Following the standard for regression analyses with random forests, the model is established using p/3 predictors (4/3~=1). To mark the predictive performance of the model, I computed the mean squared residual of the model on the test data. The initial random forest model produced an MSE of 11.625. Now, I will tune the random forest model using the caret package and compute its MSE:

```{r}
library(caret)

set.seed(35)
RF <- train(EP~., 
            data=ccpp, 
            subset=sel,
            method="rf",
            trControl=trainControl(method="cv",number=3),
            tuneGrid=expand.grid(mtry=seq(1,4)))

RF$finalModel
RF_pred <- predict(RF$finalModel, newdata=ccpp[-sel,])
RF_mse <- mean((RF_pred-ccpp[-sel,"EP"])^2)
RF_mse
```

As anticipated, the parameter tuning produced a better result for the tuned random forest model than the initial model; the MSE was reduced to 10.970. These predictive model results can be displayed via the plot below:

```{r}
plot(ccpp[-sel,"EP"], RF_pred, type="p", main="Tuned Random Forest Results",
     xlab="Predicted Power Output", ylab="Observed Power Output")
abline(a = c(0,1), col="red")
```

This plot demonstrates that the vast majority of points are very close in magnitude to their predicted values, a calculation which supports the use of parameter tuning to improve ranfom forest models. 


We will now investigate the performance of a tuned extreme gradient boosting model on the same data set:

```{r}
library(xgboost)

xtrain = xgb.DMatrix(as.matrix(ccpp[sel,] %>% select(-EP), rownames.force = F))
ytrain = ccpp[sel,"EP"]
xtest = xgb.DMatrix(as.matrix(ccpp[-sel,] %>% select(-EP), rownames.force = F))
ytest = ccpp[-sel,"EP"]

set.seed(35)
XG <- train(x = xtrain, 
            y = ytrain,
            subset=sel,
            method="xgbTree",
            trControl=trainControl(method="cv",number=3),
            tuneGrid=expand.grid(eta=c(0.1, 0.3, 0.7), 
                                 max_depth = c(1, 10, 20), 
                                 gamma = 10, 
                                 subsample = c(0.5, 1), 
                                 nrounds = 200, 
                                 colsample_bytree = c(0.5, 0.75, 0.9), 
                                 min_child_weight = 1))
            
XG_pred <- predict(XG, newdata=xtest)
XG_mse <- mean((XG_pred-ytest)^2)
print(XG_mse)
```

By tuning the extreme gradient boosting model according to the tuneGrid parameters, the model was able to achieve a test set MSE of 9.724. This marks a substantial improvement in performance to the initial random forest model and the tuned ranfom forest model as well. 

These tabular results can also be displayed visually using the plot below:

```{r}
plot(ytest, XG_pred, type="p", main="Tuned XGBoost Results",
     xlab="Predicted Power Output", ylab="Observed Power Output")
abline(a = c(0,1), col="red")

```

With the exception of a few outliers, the extreme gradient boosting model performed very well on the test set, with the vast majority of points aligning with the line of their predicted value. 


# Question 3

> *Consider again the CCPP data described above, but modify the data so that response (PE) is binary as follows: if PE is < 450, response is low and if PE >= 450, response is high. Analyze this data using adaptive boosting (ada). Create training and test data sets, tune your trained models using caret and evaluate test performance in both tabular and graphical formats.*

To accomplish this, I will re-code the data set to account for the binary response variable and create an adaptive boosting model using the ada package:

```{r}
library(ada)

b_ccpp <- ccpp %>%
    mutate(EP = ifelse(EP<450,"low", "high"))

set.seed(39)
sel <- sample(nrow(b_ccpp),nrow(b_ccpp)*0.6)

AD <- train(as.factor(EP)~., 
            data=b_ccpp, 
            subset=sel,
            method="ada",
            trControl=trainControl(method="cv",number=3),
            tuneGrid=expand.grid(iter=c(10, 50), 
                                 maxdepth = c(1,5),
                                 nu=c(0.1,1)))
AD$bestTune

```

As we can see, the model with the best results was that with 50 iterations, a max tree depth of 5, and a nu of 0.1. The results of the model's performance can be visualized using the confusion matrix below:

```{r}
AD_pred <- predict(AD, b_ccpp[-sel,])

table(AD_pred, b_ccpp[-sel,"EP"])
sum(AD_pred != b_ccpp[-sel, "EP"])/length(AD_pred)
```

This tuned adaptive boosting model produced a test data mislcassification error of 0.0452, a very good result for a problem of this magnitude. The confusion matrix shows that the vast majority of observations in the test set are being correctly classified, but some results are present in the off-diagonal elements. The model can also be visualized using the ROC curve below:

```{r}
library(ROCR)
library(ggplot2)

pr <- predict(AD,newdata=b_ccpp[-sel,],type="prob")[,2]
pred <- prediction(pr,b_ccpp[-sel,"EP"])
pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve - Adaptive Boosting [test]")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,label=paste("AUC =", round(au, 2)))
print(p)
```

When matched with the test data, the adaptive boosting model produces an area under the curve of 0.99, which is nearly perfect. This represents the very low tendency of the model to misclassify the electricity production as high or low. These powerful results follow intuition, as transforming electricity produciton into a binary cateogrical response variable should make for a very accurate model, especially considering the accuracy of boosting methods for the continuous response. 








